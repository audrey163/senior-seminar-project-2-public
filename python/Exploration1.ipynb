{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631814dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pprint as pp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "from regression import PolynomialLibrary, TrigLibrary\n",
    "import sindy_helper\n",
    "from dynamicalsystems import TrainDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26ae1d6-c138-492d-81ca-2e4f246c67bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still debuging FullSINDyAutoencoder.get_dX I set the reg['dX'] = 0 in the loss args\n"
     ]
    }
   ],
   "source": [
    "print(\"Still debuging FullSINDyAutoencoder.get_dX I set the reg['dX'] = 0 in the loss args\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d168b2fb-4757-4d14-9bff-aa6cb2f3b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullSINDyAutoencoder(nn.Module):\n",
    "    def __init__(self,batch_size, num_features):\n",
    "        super().__init__()\n",
    "        self.num_snapshots, self.num_features = batch_size, 2\n",
    "        self.Theta = sindy_helper.Theta(self.num_snapshots, self.num_features)\n",
    "        self.theta = self.Theta.theta\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(10, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, self.num_features) # -> N, 3\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.num_features,4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8,10)\n",
    "        )\n",
    "        self.SINDy_layer = nn.Sequential(\n",
    "            nn.Linear(len(self.Theta.candidate_terms),self.num_features,bias=False)\n",
    "        )\n",
    "    def forward(self, x, dx):\n",
    "        Z = self.encoder(x)\n",
    "        dZ = self.get_dZ(x,dx)\n",
    "        X_pred = self.decoder(Z)\n",
    "        dZ_pred = self.sindy(Z)\n",
    "        #dX_pred = self.get_dX(Z,dZ)\n",
    "        Xi = self.SINDy_layer[0].weight\n",
    "        #This is what I need I dont know how to get Xi = Weghts from self.SINDy_layer = nn.Linear..\n",
    "        return { 'X' : x, 'dX' : dx,'Z' : Z, 'dZ' : dZ,'X_pred' : X_pred, 'dZ_pred' : dZ_pred, 'Xi' : Xi} #'dX_pred' : dX_pred\n",
    "    def get_dZ(self,x,dx):\n",
    "        J = torch.autograd.functional.jacobian(self.encoder, x)\n",
    "        return torch.matmul(J.T,dx)\n",
    " \n",
    "    def get_dX(self,z,dz):\n",
    "        J = torch.autograd.functional.jacobian(self.decoder, z)\n",
    "        return torch.matmul(J.T,dz)\n",
    "\n",
    "    def sindy(self,Z): #HW 1\n",
    "         theta_Z = self.theta(Z)\n",
    "         # Z_dot_predict = f(Z) = Θ(Z)Ξ = Θ(Z)[ ξ1, ξ2, ..., ξn ]\n",
    "         return self.SINDy_layer(theta_Z)\n",
    "\n",
    "    def loss(self,args,reg):\n",
    "        return reg['X']*torch.linalg.norm(args['X'] - args['X_pred']) \n",
    "        + reg['SINDy']*torch.linalg.norm(args['dZ_pred'] - args['dZ'])\n",
    "        + reg['dX']*torch.linalg.norm(args['dX_pred'] - args['dX'])\n",
    "        + reg['Xi']*torch.linalg.norm(args['Xi'],ord=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520846fb-2369-4b4a-9365-b9992b5328d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_pers = {\n",
    "    'learn_rate' : 1e-3,\n",
    "    'batch_size' : 64,\n",
    "    'num_epochs' : 1000,\n",
    "    'num_features' : 2,\n",
    "    'loss_reg' : {\n",
    "        'X' : 1,\n",
    "        'SINDy' : 1,\n",
    "        'dX' : 0, \n",
    "        'Xi' : 1\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset = TrainDataset()\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=hyper_pers['batch_size'], shuffle=True, num_workers=2)\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "model = FullSINDyAutoencoder(hyper_pers['batch_size'], hyper_pers['num_features'])\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=hyper_pers['learn_rate'])\n",
    "\n",
    "total_samples = len(dataset)\n",
    "n_iter = math.ceil(total_samples / hyper_pers['batch_size'])\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633406c-3cfb-40b5-87d5-342814977382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23224/1459347464.py:35: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2318.)\n",
      "  return torch.matmul(J.T,dx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.220434188842773\n",
      "Epoch 1\n",
      "16.029891967773438\n",
      "Epoch 2\n",
      "14.413824081420898\n",
      "Epoch 3\n",
      "14.583720207214355\n",
      "Epoch 4\n",
      "14.573553085327148\n",
      "Epoch 5\n",
      "13.945926666259766\n",
      "Epoch 6\n",
      "13.695379257202148\n",
      "Epoch 7\n",
      "9.892730712890625\n",
      "Epoch 8\n",
      "1.736830472946167\n",
      "Epoch 9\n",
      "1.0449066162109375\n",
      "Epoch 10\n",
      "0.6116343140602112\n",
      "Epoch 11\n",
      "0.7608805298805237\n",
      "Epoch 12\n",
      "0.597195565700531\n",
      "Epoch 13\n",
      "0.3743334114551544\n",
      "Epoch 14\n",
      "0.6218633651733398\n",
      "Epoch 15\n",
      "0.21290361881256104\n",
      "Epoch 16\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(hyper_pers['num_epochs']):\n",
    "    print(\"Epoch \" + str(epoch))\n",
    "    l = []\n",
    "    for i, (X, dX) in enumerate(dataloader):\n",
    "        if X.shape[0] == hyper_pers['batch_size']:\n",
    "            res = model.forward(X,dX)\n",
    "            loss = model.loss(args=res,reg=hyper_pers['loss_reg'])   \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            l.append(loss.item())\n",
    "    losses.append(np.mean(np.array(l)))\n",
    "    print(loss.item())\n",
    "    \n",
    "plt.plot(np.log(np.array(losses)))\n",
    "plt.title(\"log loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
