{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631814dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pprint as pp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "from regression import PolynomialLibrary, TrigLibrary\n",
    "import sindy_helper\n",
    "from dynamicalsystems import TrainDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d168b2fb-4757-4d14-9bff-aa6cb2f3b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullSINDyAutoencoder(nn.Module):\n",
    "    def __init__(self,batch_size, num_features):\n",
    "        super().__init__()\n",
    "        self.num_snapshots, self.num_features = batch_size, 2\n",
    "        self.Theta = sindy_helper.Theta(self.num_snapshots, self.num_features)\n",
    "        self.theta = self.Theta.theta\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(10, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, self.num_features) # -> N, 3\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.num_features,4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8,10)\n",
    "        )\n",
    "        self.SINDy_layer = nn.Linear( \n",
    "            len(self.Theta.candidate_terms),\n",
    "            self.num_features, \n",
    "            bias=False\n",
    "        )\n",
    "    def forward(self, x, dx):\n",
    "        Z = self.encoder(x)\n",
    "        dZ = self.get_dZ(x,dx)\n",
    "        X_pred = self.decoder(Z)\n",
    "        dZ_pred = self.sindy(Z)\n",
    "        \n",
    "        #This is what I need I dont know how to get Xi = Weghts from self.SINDy_layer = nn.Linear..\n",
    "        return { 'X' : x, 'dX' : dx,'X_pred' : X_pred, 'dZ_pred' : dZ_pred, 'Z' : Z, 'dZ' : dZ}\n",
    "        #Xi = 0\n",
    "        #dX_pred = self.get_dX(Z) #HW audrey\n",
    "        #return { 'X' : x, 'dX' : dx,'X_pred' : X_pred, 'dZ_pred' : dZ_pred, 'Z' : Z, 'dZ' : dZ, 'Xi' : Xi}\n",
    "    def get_dZ(self,x,dx):\n",
    "        J = torch.autograd.functional.jacobian(self.encoder, x)\n",
    "        return torch.matmul(J.T,dx)\n",
    " \n",
    "    def get_dX(self,z,dz):\n",
    "        J = torch.autograd.functional.jacobian(self.decoder, z)\n",
    "        return torch.matmul(J.T,dz)\n",
    "\n",
    "    def sindy(self,Z): #HW 1\n",
    "         theta_Z = self.theta(Z)\n",
    "         # Z_dot_predict = f(Z) = Θ(Z)Ξ = Θ(Z)[ ξ1, ξ2, ..., ξn ]\n",
    "         return self.SINDy_layer(theta_Z)\n",
    "\n",
    "    def loss(self,args):\n",
    "        return torch.linalg.norm(args['X'] - args['X_pred']) \n",
    "        + torch.linalg.norm(args['dZ_pred'] - args['dZ'])\n",
    "        + torch.linalg.norm(args['dX_pred'] - args['dX'])\n",
    "        #+ torch.linalg.norm(args['Xi'],ord=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520846fb-2369-4b4a-9365-b9992b5328d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_features = 64, 2\n",
    "\n",
    "dataset = TrainDataset()\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "model = FullSINDyAutoencoder(batch_size, num_features)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "num_epochs = 1000\n",
    "total_samples = len(dataset)\n",
    "n_iter = math.ceil(total_samples / batch_size)\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633406c-3cfb-40b5-87d5-342814977382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10577/1211376975.py:39: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2318.)\n",
      "  return torch.matmul(J.T,dx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.245515823364258\n",
      "Epoch 1\n",
      "11.969353675842285\n",
      "Epoch 2\n",
      "6.004354000091553\n",
      "Epoch 3\n",
      "1.2372723817825317\n",
      "Epoch 4\n",
      "0.2335112988948822\n",
      "Epoch 5\n",
      "0.27256959676742554\n",
      "Epoch 6\n",
      "0.15802410244941711\n",
      "Epoch 7\n",
      "0.12492448091506958\n",
      "Epoch 8\n",
      "0.11344097554683685\n",
      "Epoch 9\n",
      "0.1211489886045456\n",
      "Epoch 10\n",
      "0.19380328059196472\n",
      "Epoch 11\n",
      "0.16794204711914062\n",
      "Epoch 12\n",
      "0.11651460826396942\n",
      "Epoch 13\n",
      "0.09825518727302551\n",
      "Epoch 14\n",
      "0.12674292922019958\n",
      "Epoch 15\n",
      "0.06544368714094162\n",
      "Epoch 16\n",
      "0.061580270528793335\n",
      "Epoch 17\n",
      "0.05066531151533127\n",
      "Epoch 18\n",
      "0.07795917987823486\n",
      "Epoch 19\n",
      "0.12024378031492233\n",
      "Epoch 20\n",
      "0.15622319281101227\n",
      "Epoch 21\n",
      "0.0817042663693428\n",
      "Epoch 22\n",
      "0.14189444482326508\n",
      "Epoch 23\n",
      "0.08717077970504761\n",
      "Epoch 24\n",
      "0.06220019981265068\n",
      "Epoch 25\n",
      "0.21312744915485382\n",
      "Epoch 26\n",
      "0.15096500515937805\n",
      "Epoch 27\n",
      "0.3048502802848816\n",
      "Epoch 28\n",
      "0.06477982550859451\n",
      "Epoch 29\n",
      "0.07420310378074646\n",
      "Epoch 30\n",
      "0.0946212187409401\n",
      "Epoch 31\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch \" + str(epoch))\n",
    "    l = []\n",
    "    for i, (X, dX) in enumerate(dataloader):\n",
    "        if X.shape[0] == batch_size:\n",
    "            res = model.forward(X,dX)\n",
    "            loss = model.loss(res)   \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            l.append(loss.item())\n",
    "    losses.append(np.mean(np.array(l)))\n",
    "    print(loss.item())\n",
    "    \n",
    "plt.plot(np.log(np.array(losses)))\n",
    "plt.title(\"log loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
